{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from plots import *\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 30), (5000,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=y.reshape(len(y),1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ..., \n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code section, we choose how do we deal with the datas. This part is extremly important since the method we choose is going to change the predictions. We have roughly two methods to deal with outliers, we tried both and we are going to compare them.\n",
    "\n",
    "The method function returns the matrix we are going to work with. If argument 1 is chosen, we will use the first method and with argument 2 the second method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First method\n",
    "We remove all columns containing -999. using remove_outlier_columns function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tX, new_mean, new_std = method(1,tX)\n",
    "new_tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second method\n",
    "We transform all the -999. values by the mean of the columns (calculated without counting the -999.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   3.07797572e-01,   3.96543618e-02, ...,\n",
       "          1.13700709e+00,  -2.39839608e+00,   3.98182143e-01],\n",
       "       [  1.00000000e+00,   1.78768046e+00,   6.33440608e-01, ...,\n",
       "         -1.02859606e-16,   4.80781369e-16,  -2.31796107e-01],\n",
       "       [  1.00000000e+00,  -5.67586021e-01,  -6.41518864e-01, ...,\n",
       "         -1.02859606e-16,   4.80781369e-16,  -1.08298366e-01],\n",
       "       ..., \n",
       "       [  1.00000000e+00,   3.78993993e-01,  -1.22366112e+00, ...,\n",
       "         -1.02859606e-16,   4.80781369e-16,   2.34160770e-01],\n",
       "       [  1.00000000e+00,  -7.99685985e-01,  -1.12174093e-01, ...,\n",
       "         -1.02859606e-16,   4.80781369e-16,  -4.11062512e-01],\n",
       "       [  1.00000000e+00,   1.53027534e-01,  -1.31829705e+00, ...,\n",
       "         -1.02859606e-16,   4.80781369e-16,  -2.25396588e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_tX, mean_mean, mean__std = method(2,tX)\n",
    "mean_tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each method 1 and 2, we will implement the 6 ML methods asked in the step 2 of the project description and will compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dup_tX, dup_mean, dup_std= method(3, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 91)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "### with gradient descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the parameters of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters = 501\n",
    "gamma = 0.04\n",
    "#initial_w1 = np.zeros((new_tX.shape[1], 1))\n",
    "#initial_w2 = np.zeros((mean_tX.shape[1], 1))\n",
    "initial_w3 = np.zeros((dup_tX.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: We remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/500): loss=0.5\n",
      "Gradient Descent(100/500): loss=0.34664069018483407\n",
      "Gradient Descent(200/500): loss=0.3420824627689666\n",
      "Gradient Descent(300/500): loss=0.34050984134672735\n",
      "Gradient Descent(400/500): loss=0.33992347569151465\n",
      "Gradient Descent(500/500): loss=0.33969186313755456\n",
      "Gradient Descent(500/500): loss=0.33969186313755456\n",
      "Gradient Descent: execution time=0.118 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "best_ws1= least_square_GD(y, new_tX, initial_w1, gamma,max_iters)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: We change outliers with mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/500): loss=0.5\n",
      "Gradient Descent(100/500): loss=0.3338048617023286\n",
      "Gradient Descent(200/500): loss=0.32988750586239085\n",
      "Gradient Descent(300/500): loss=0.3285278951623448\n",
      "Gradient Descent(400/500): loss=0.3279773616673182\n",
      "Gradient Descent(500/500): loss=0.32773906243490547\n",
      "Gradient Descent(500/500): loss=0.32773906243490547\n",
      "Gradient Descent: execution time=0.139 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "best_ws2 = least_square_GD(y, mean_tX, initial_w2, gamma,max_iters)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with stochastique gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 21\n",
    "batch_size=1\n",
    "initial_w1 = np.zeros((new_tX.shape[1], 1))\n",
    "initial_w2 = np.zeros((mean_tX.shape[1], 1))\n",
    "\n",
    "# For the moment we use this gamma\n",
    "gamma = 0.0004\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/20): loss=0.3535142666213228\n",
      "Gradient Descent(10/20): loss=0.34023996639823273\n",
      "Gradient Descent(20/20): loss=0.34031509649490976\n",
      "SGD: execution time=12.530 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_w  = least_square_SGD(y, new_tX, batch_size, initial_w1, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/20): loss=0.34021643318913264\n",
      "Gradient Descent(10/20): loss=0.32851879948084595\n",
      "Gradient Descent(20/20): loss=0.3288403940775238\n",
      "SGD: execution time=13.672 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_w  = least_square_SGD(y, mean_tX, batch_size, initial_w2, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least square method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n",
      "0.339347093912\n"
     ]
    }
   ],
   "source": [
    "ls_weight1, ls_mse1= least_squares(y,new_tX)\n",
    "print(ls_weight1.shape)\n",
    "print(ls_mse1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 1)\n",
      "0.327358937889\n"
     ]
    }
   ],
   "source": [
    "ls_weight2, ls_mse2= least_squares(y,mean_tX)\n",
    "print(ls_weight2.shape)\n",
    "print(ls_mse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 1)\n",
      "0.284985490011\n"
     ]
    }
   ],
   "source": [
    "ls_weight3, ls_mse3= least_squares(y,dup_tX)\n",
    "print(ls_weight3.shape)\n",
    "print(ls_mse3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define lambda parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_= 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 1), 0.084879941509701171)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr_w1, rr_mse1 = ridge_regression(y, new_tX, lambda_)\n",
    "rr_w1.shape, rr_mse1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31, 1), 0.081885074116411716)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr_w2, rr_mse2 = ridge_regression(y, mean_tX, lambda_)\n",
    "rr_w2.shape, rr_mse2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only for logistic regression, we change all -1 of y variable into 0, so that we can reuse all the functions we have already implemented for the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y[y==-1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### with gradient descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters_gr = 1501\n",
    "alpha_gr = 0.00007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call logistic regression with gradient descent -> with true argument "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=3465.735902799727\n",
      "Current iteration=200, the loss=2519.935860886566\n",
      "Current iteration=400, the loss=2503.971027748691\n",
      "Current iteration=600, the loss=2500.5371291791503\n",
      "Current iteration=800, the loss=2499.6369512030624\n",
      "Current iteration=1000, the loss=2499.3769424022385\n",
      "Current iteration=1200, the loss=2499.2979090108206\n",
      "Current iteration=1400, the loss=2499.2731955062186\n",
      "The loss=2499.268177831087\n",
      "SGD: execution time=2.432 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "log_loss1,log_w1 = logistic_regression(y, new_tX, initial_w1, alpha_gr, max_iters_gr, True)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=3465.735902799727\n",
      "Current iteration=200, the loss=2441.741568859311\n",
      "Current iteration=400, the loss=2427.069526851878\n",
      "Current iteration=600, the loss=2423.3358966891474\n",
      "Current iteration=800, the loss=2422.1111050467716\n",
      "Current iteration=1000, the loss=2421.6670754042498\n",
      "Current iteration=1200, the loss=2421.4979986190306\n",
      "Current iteration=1400, the loss=2421.431848726739\n",
      "The loss=2421.4157403313693\n",
      "SGD: execution time=3.294 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "log_loss2,log_w2 = logistic_regression(y, mean_tX, initial_w2, alpha_gr, max_iters_gr, True)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=3465.735902799727\n",
      "Current iteration=200, the loss=2197.3709737807376\n",
      "Current iteration=400, the loss=2135.783330433328\n",
      "Current iteration=600, the loss=2100.2465846756886\n",
      "Current iteration=800, the loss=2076.051036054824\n",
      "Current iteration=1000, the loss=2058.3854434151262\n",
      "Current iteration=1200, the loss=2044.886063344501\n",
      "Current iteration=1400, the loss=2034.210845451194\n",
      "The loss=2029.6619393505387\n",
      "SGD: execution time=12.225 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "log_loss2,log_w2 = logistic_regression(y, dup_tX, initial_w3, alpha_gr, max_iters_gr, True)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters_newt = 201\n",
    "alpha_newt = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call logistic regression with newton method --> with false argument "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=3465.735902799727\n",
      "Current iteration=200, the loss=2498.1164969445135\n",
      "Current iteration=400, the loss=2498.102099654334\n",
      "Current iteration=600, the loss=2498.102099580248\n",
      "Current iteration=800, the loss=2498.102099580261\n",
      "Current iteration=1000, the loss=2498.102099580381\n",
      "The loss=2498.102099580381\n"
     ]
    }
   ],
   "source": [
    "# Start Newton method.\n",
    "start_time = datetime.datetime.now()\n",
    "l,w = logistic_regression(y, new_tX, initial_w1, alpha_newt, max_iters_newt, False)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (250000,1) (5000,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-e6bb1bedf309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start Newton method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_tX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_newt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters_newt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gusto/ML_project1/helpers.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, initial_w, alpha, max_iters, method)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# get loss and and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_aid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# We print loss each 200 iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gusto/ML_project1/helpers.py\u001b[0m in \u001b[0;36mlogistic_regression_aid\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;34m\"\"\"return the loss, gradient and hessian !\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcalculate_logistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_logistic_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_logistic_hessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gusto/ML_project1/helpers.py\u001b[0m in \u001b[0;36mcalculate_logistic_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mlogistic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (250000,1) (5000,1) "
     ]
    }
   ],
   "source": [
    "# Start Newton method.\n",
    "start_time = datetime.datetime.now()\n",
    "l,w = logistic_regression(y, mean_tX, initial_w2, alpha_newt, max_iters_newt, False)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=173286.79513998624\n",
      "Current iteration=200, the loss=106224.88364393148\n",
      "The loss=106224.88364393148\n",
      "SGD: execution time=98.183 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start Newton method.\n",
    "start_time = datetime.datetime.now()\n",
    "l,w = logistic_regression(y, dup_tX, initial_w3, alpha_newt, max_iters_newt, False)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalized Logistic Regression\n",
    "### with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "alpha = 0.0004\n",
    "lambda_ = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=3465.735902799727\n",
      "The loss=2555.967870559867\n"
     ]
    }
   ],
   "source": [
    "reg_loss1, reg_w2 = reg_logistic_regression(y, new_tX, initial_w1, lambda_, alpha, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=3465.735902799727\n",
      "The loss=2482.690875143767\n"
     ]
    }
   ],
   "source": [
    "reg_loss2, reg_w2 = reg_logistic_regression(y, mean_tX, initial_w2, lambda_, alpha, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "## Ploting on vairious lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reshape_y=y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1666)\n",
      "(5000, 1)\n",
      "(5000, 20)\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1706.2500256988496\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1721.4391549146555\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1695.6091645731997\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1706.2500332756122\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1721.4391623837732\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1695.609172256777\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1706.2524292428052\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1721.4415243107444\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1695.6116020015947\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1707.008558594978\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1722.186892892579\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1696.378401406184\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1856.11009093617\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1867.6814418032386\n",
      "Current iteration=0, the loss=2310.9526999868576\n",
      "The loss=1847.91032444055\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEdCAYAAADJporJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRDJTMRERkcxUTEREJDMVExERyez/A/j9kpgZX4C2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x137bd2b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cross_validation_demo(seed, initial_w, alpha, max_iter, degree,k_fold,lambdas):\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    print(k_indices.shape)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    rmse_te_var=[]\n",
    "\n",
    "    # cross validation: TODO\n",
    "    print(y.shape)\n",
    "    print(new_tX.shape)\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        loss_tr, loss_te, rmses_test = cross_validation(y, new_tX, initial_w1, alpha, max_iter, k_indices, k_fold, lambda_, degree)\n",
    "        rmse_tr.append(loss_tr)\n",
    "        rmse_te.append(loss_te)\n",
    "        rmse_te_var.append(rmses_test)\n",
    "    \n",
    "    # ***************************************************    \n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    \n",
    "     # Create a figure instance\n",
    "    #fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "    # Create an axes instance\n",
    "    #ax = fig.add_subplot(111)\n",
    "\n",
    "    # Create the boxplot\n",
    "    #bp = ax.boxplot(rmse_te_var)\n",
    "\n",
    "cross_validation_demo(2000, initial_w1, alpha, max_iter, 1, 3, np.logspace(-8, 2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la mthode 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_tX_test, mean_mean, mean__std = standardize(undefToMeanMean(tX_test), mean_x=None, std_x=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la mthode 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup_tX_test,dup_mean, dup_std = standardize(build_poly(undefToMeanMean(tX_test),3), mean_x=None, std_x=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 91)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'dup_tX_newton.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, dup_tX_test, logistic= True)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.20967426],\n",
       "       [-2.57229167],\n",
       "       [-1.42675124],\n",
       "       ..., \n",
       "       [-0.25694514],\n",
       "       [-0.02774611],\n",
       "       [-2.87362728]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(mean_tX_test, reg_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
